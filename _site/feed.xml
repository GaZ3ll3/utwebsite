<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome to Yimin's Homepage</title>
    <description>Academic homepage of Yimin</description>
    <link>http://www.ma.utexas.edu/users/yzhong</link>
    <atom:link href="http://www.ma.utexas.edu/users/yzhong/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Billiards</title>
        <description>&lt;p&gt;It is an old topic and until today I finally implemented it. On a heterogeneous 2D disk, we consider geodesics in phase space according to Hamiltonian as $H = c^2 |\xi|^2$, which depicts the movement of a point billiard traveling inside the disk, $H$ can be seen as some mountains for potential.&lt;/p&gt;

&lt;p&gt;To fully recover the information of speed $c(x)$, we will need the scatter relation $(X_s, X_r, t_{sr})$, which is a tuple of input phase coordinate, output phase coordinate and traveltime. In theory, $X_s$ and $X_r$ do not need to be located on the boundary, but in practice, we set sources are equi-spaced located on circle, with equi-spaced angles of input, and we record traveltime and output location, angle on the boundary.&lt;/p&gt;

&lt;p&gt;So the data we have is described by a $N_s N_a \times 9$ matrix, each row containns information about a ray. Numerically, the disk is discretized on grid, so we need to let each grid passed by at least one ray. And it is quite easy to see that error will accumulate exponentially (Since it is simply a time integral).&lt;/p&gt;

&lt;p&gt;For reconstruction of speed $c(x)$, once we use linearization, we will arrive at a linear system, but time integral related. We have to solve a non-coupled time integral.&lt;/p&gt;

&lt;p&gt;$$\frac{dX}{ds} = L(X), \quad X(0) = X_s,\quad X(t_{sr}) = X_r$$
where $L = \mathrm{diag}(1,-1)\partial H/\partial X$. And Jacobian $\partial X/\partial X(0)$ satisfies
$$\frac{dJ}{ds} = M(X)J,\quad J(0) = I$$&lt;/p&gt;

&lt;p&gt;which of course forms a one-parameter group. We immediately get&lt;/p&gt;

&lt;p&gt;$$X_r - X_s \simeq \int_0^{t_{sr}} J(t_{sr}) J(t)^{-1} S(\delta c, X(t))dt $$
where $S(f, X)$ is linear operator in $f$, for each phase coordinate $X$. Thus by trapezoid rule and take matrix form of $S$,&lt;/p&gt;

&lt;p&gt;$$X_r - X_s = J(t_{sr})(\sum_j w_j \Delta t J(t)^{-1} S_j) \delta c $$&lt;/p&gt;

&lt;p&gt;$w_j$ are weights. And using all scattering pairs, we have a linear system as
$$D_{4m\times N} = A_{4m\times N} \delta c$$&lt;/p&gt;

&lt;p&gt;As we have seen from previous discussion, matrix $A$ has rank ordering property, which means some pairs involves less unknowns, some pairs are involving more unknowns of $\delta c$. So we can reorder the measurement $D$ and $A$ according to the rank &lt;code&gt;nnz(A(k, :))&lt;/code&gt;, in MATLAB, just use &lt;code&gt;colperm(A&amp;#39;)&lt;/code&gt; to get the ordering. It is not a surprise that, the longest ray maybe involve almost all unknowns.&lt;/p&gt;

&lt;h2 id=&quot;toc_0&quot;&gt;Foliation&lt;/h2&gt;

&lt;p&gt;We can show the continuity of dependence of $X_r$ on $X_s$, which is trivial. We need to take small input angle to make sure dependence is minimal, i.e. ray only passes through a minimal number of grids, although, passing through one grid will automatically involve 12 grid points due to gradient in $S$. Thus, it is not numerically possible to do the problem on fine grid, therefore, fine resolution is not doable.&lt;/p&gt;

&lt;p&gt;Roughly speaking, for boundary grids, there are 12 values involved for one grid, thus, the safe option for angular discretization is use at least 24 rays at each boundary point to recover boundary values. For interior, it depends, the most central grid needs to be resolved by rays, i.e. strongly/weakly passing through.&lt;/p&gt;

&lt;p&gt;Foliation should be done automatically if we have the matrix $A$, rearranged as
$$A = [A_1; A_2; A_3; ...]$$&lt;/p&gt;

&lt;p&gt;Where $A_k$ are smaller sparse matrices and of low rank, means corresponding $D_k = A_k \delta c$ is possible to solve for some values of $\delta c$. This process is called &amp;quot;Foliation&amp;quot;.&lt;/p&gt;

&lt;p&gt;The algorithm can be stated easily (noiseless).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Consider index set, $S_{1}$ and $S_2$, put all non-dependent grid values in $S_2$, since they are known.&lt;/li&gt;
&lt;li&gt;only consider $S_1$ grids, rearrange $A$ by &lt;code&gt;nnz&lt;/code&gt; of each row, ordering is $p$.&lt;/li&gt;
&lt;li&gt;solve part of the problem $D(p(1:s)) = A(p(1:s)) \delta c$ for a selected $s$, and move this part of index from $S_1$ to $S_2$.&lt;/li&gt;
&lt;li&gt;Run forward problem and get new data.&lt;/li&gt;
&lt;li&gt;If $S_1$ is non-empty, goto 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;toc_1&quot;&gt;Complexity&lt;/h2&gt;

&lt;p&gt;The linear scheme can give at most first order convergence, all timing should be linear. Most time cost are spent on solving the rays, which is a 20-dimensional problem. A fast parallel solver is required for 1000 measurement or above.&lt;/p&gt;

&lt;p&gt;In MATLAB, &lt;code&gt;ode45&lt;/code&gt; or &lt;code&gt;ode23&lt;/code&gt; can be used as time integrator, but it kills flexibility if we need to calculate matrix $A$ at the same time without memorizing everything, one should implement &lt;code&gt;rk4&lt;/code&gt; for this purpose.&lt;/p&gt;

&lt;p&gt;It will be quite slow for sure, the matrix manipulation are not simple ones.&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/10/10/billiards/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/10/10/billiards/</guid>
      </item>
    
      <item>
        <title>Wignerization</title>
        <description>&lt;p&gt;Though it is tedious, I would think some notes on Wigner transform is necessary to some beginner like me. All notes are based on H.T. Yau&amp;#39;s famous result in 1999.&lt;/p&gt;

&lt;p&gt;For Schrodinger equation, $i\psi_t = H\psi$, where $H = H_0 + \lambda V$, $H_0 = -\frac{1}{2}\Delta$. Formally, we have Dyson series&lt;/p&gt;

&lt;p&gt;$$\psi_t = e^{-itH}\psi_0 = \sum_{n=0}^{N-1} \psi_n(t) + \Psi_N(t)$$
where $V$ is spatially variable,
$$\psi_n(t) = (-i\lambda)^n (\int_0^t)^{n+1} \prod_{j=0}^n(ds_j) \delta(t - \sum_{j=0}^n s_j) e^{-is_0H_0}V\cdots Ve^{-is_n H_0}  \psi_0$$
and remainder is
$$\Psi_N(t) = -i\lambda \int_0^t ds e^{-i(t-s)H}V\psi_{N-1}$$&lt;/p&gt;

&lt;p&gt;We take Fourier transform on $\psi_n$, ignoring the constants $(2\pi)^d$ by normalizing the Lebesgue measure. The integral is actually in a simplex instead of full cube.&lt;/p&gt;

&lt;p&gt;$$\widehat{\psi}_n(t, p_0) = \lambda^n \int \prod_{j=1}^n (dp_j) K(t;p,n) L(p, n) \widehat{\psi}_0(p_n)$$&lt;/p&gt;

&lt;p&gt;where $$K = (-i)^n \int_0^t \prod_{j=0}^n (ds_j) \delta(t - \sum_{j=0}^n s_j) \prod_{j=0}^n e^{-is_j p_j^2}$$ $$L(p,n) = \prod_{j=0}^{n-1} \widehat{V}(p_j - p_{j+1})$$ It is straightforward to check this by convolution back.&lt;/p&gt;

&lt;p&gt;The main result shows that if we truncate the series, we can get error term goes away by taking limit.&lt;/p&gt;

&lt;p&gt;For $\psi = \psi_1 + \psi_2$, with $\psi_2$ as error, we can show that for any admissible functional $\widehat{J}_{\epsilon}(\xi, v)$,
$$|E\int \overline{\widehat{J}_{\epsilon}}(\xi, v)(\widehat{W}_{\psi}(\xi, v) - \widehat{W}_{\psi_1}(\xi, v))| d\xi dv\le C \sup|\widehat{J}_{\epsilon}|_{1,\infty}\sqrt{E|\psi_2|^2 E|\psi_1|^2}$$&lt;/p&gt;

&lt;p&gt;Thus after truncation of Dyson series, above limit will be zero as $N\to\infty$, which means weak convergence.&lt;/p&gt;

&lt;p&gt;The Wignerization of the truncated series will involve all terms like
$$ E \int \overline{\widehat{J}}_{\epsilon} \overline{\widehat{\psi}_n(t, v-\frac{\xi}{2})} \widehat{\psi}_{m}(t, v + \frac{\xi}{2}) d\xi dv$$
above expectation involves $L(p,n), L(p&amp;#39;,m)$, when $V$ is standard Gaussian process, then $n+m$ must be even to get perfect pairing to be nonzero.&lt;/p&gt;

&lt;p&gt;Knowing that $E \overline{\widehat{V}(p)}\widehat{V}(q) = \delta(p - q) R(p)R(q)$, we will try to find out all cases like&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$\widehat{V}(p_j - p_{j+1})\widehat{V}(p&amp;#39;_{k} -p&amp;#39;_{k+1})$&lt;/li&gt;
&lt;li&gt;$\widehat{V}(p_j - p_{j+1})\widehat{V}(p_{k} -p_{k+1})$&lt;/li&gt;
&lt;li&gt;$\widehat{V}(p&amp;#39;_j - p&amp;#39;_{j+1})\widehat{V}(p&amp;#39;_{k} -p&amp;#39;_{k+1})$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;where $p_0 = v-\frac{\xi}{2}$ and $p&amp;#39;_0 = v + \frac{\xi}{2}$. Connecting all $p$ and $p&amp;#39;$ forming a loop, all relations can be classified as nested, crossing and simple. It can be shown that these kinds of interaction is dominated by simple relations.&lt;/p&gt;

&lt;p&gt;[TO BE CONTINUED]&lt;/p&gt;
</description>
        <pubDate>Sun, 01 Oct 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/10/01/Wigerization/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/10/01/Wigerization/</guid>
      </item>
    
      <item>
        <title>Bregman As Default?</title>
        <description>&lt;h4 id=&quot;toc_0&quot;&gt;Some verbose trash&lt;/h4&gt;

&lt;p&gt;For most nonlinear inverse problem, direct solution is not accessible easily, thus many people chose an iterative optimization scheme.&lt;/p&gt;

&lt;p&gt;Suppose forward model $\mathcal{M}: (\sigma, u_g, g)\to m$ passes unknown variable coefficient $\sigma\in \mathcal{E}$, unknown solution $u_g$ and known input $g\in\mathcal{G}$ to some observable measurement $m$, where $u$ satisfies solution model constraint $F(u_g, \sigma, g) = 0$. we usually use minimization scheme:
$$\min_{\sigma} J(\sigma, u_g) = \sum_{g\in\mathcal{G}} |\mathcal{M}(\sigma, u_g, g) - m|^q + \frac{\beta}{2}|\sigma|_{p}$$&lt;/p&gt;

&lt;p&gt;values for $p,q$ are found everywhere, common choices for regularization is TV or $L_1, L_2$.&lt;/p&gt;

&lt;p&gt;When $q=2$, adjoint method is often used to reduce computational cost, ignoring $g$ for each input,&lt;/p&gt;

&lt;p&gt;$$\frac{\partial |\mathcal{M}(\sigma,u) - m|^2}{\partial \sigma} (\delta \sigma)=2( \frac{\partial \mathcal{M}}{\partial \sigma}(\delta \sigma))^t(\mathcal{M}(\sigma,u) - m)$$
and
$$\frac{\partial \mathcal{M}(\sigma, u)}{\partial \sigma}(\delta \sigma) = \mathcal{M}_{\sigma}\delta\sigma + \mathcal{M}_{u}\frac{\partial u}{\partial \sigma}(\delta \sigma)$$&lt;/p&gt;

&lt;p&gt;we need to resolve $\frac{\partial u}{\partial \sigma}$ which is reduced from solution model that
$$F_u \frac{\partial u}{\partial \sigma}(\delta \sigma) + F_{\sigma}(\delta\sigma) = 0$$&lt;/p&gt;

&lt;p&gt;so we have
$$\frac{\partial \mathcal{M}(\sigma, u)}{\partial \sigma}(\delta \sigma) = \mathcal{M}_{\sigma}\delta\sigma + \mathcal{M}_u F_u^{-1} F_{\sigma}(\delta\sigma)$$&lt;/p&gt;

&lt;p&gt;The $F^{-1}_u$ is some solution operator, we take adjoint, then we only need solve
$$(F_{\sigma}(\delta\sigma))^t F_u^{-t} \mathcal{M}_u^t (\mathcal{M}(\sigma, u) - m)$$&lt;/p&gt;

&lt;p&gt;Now, for each iteration, we need to evaluate operations, mostly matrix-vector multiplications.&lt;/p&gt;

&lt;p&gt;$$\mathcal{M}_u^t(y), F_u^{-t}(y), F_{\sigma}(y), F^{-1}(y)$$&lt;/p&gt;

&lt;p&gt;For linear operators, we have $F_u = F$, but $F_{\sigma}$ has to be calculated separately, for large system, $F^{-1}$ is nontrivial to update fast.&lt;/p&gt;

&lt;p&gt;This is the unconstrained version. This version of numerical implementation has to calculate exact solutions to $F$ multiple times $O(1)$.&lt;/p&gt;

&lt;p&gt;In order to quantify the cost, we consider following model of time cost.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$t_{mv}$ is sparse matrix-vector multiplication time.&lt;/li&gt;
&lt;li&gt;$t_{sv}$ is the inversion time.&lt;/li&gt;
&lt;li&gt;$t_{pre}$ is precomputing time for &lt;code&gt;mv&lt;/code&gt; or &lt;code&gt;sv&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;$t_{set}$ is total intrinsic setting time for all operators, which is minimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total time for each iteration will be: $t_{set} + t_{pre} + 2t_{sv} + (n+1)t_{mv}$. When the operators are sparse (local), then $t_{mv} = O(n)$, $t_{sv} = O(n)$, but mostly ill-conditioned, preconditioner is needed, total is $O(n^2)$. In this case, it is not optimal to use the low rank compression here due to high overhead in $t_{pre}$.&lt;/p&gt;

&lt;p&gt;If $F$ is integral equation, then system is dense. $F_u^{-1}$ or $F^{-1}$ is not cheap to calculate in practice, iterative method might be utilized for such problems, in such case, low rank method can kick in.&lt;/p&gt;

&lt;p&gt;Then we look at constrained version. Optimization problem is
$$\min_{\sigma, u_g} J(\sigma, u_g) = \sum_{g\in\mathcal{G}} |\mathcal{M}(\sigma, u_g, g) - m|^q + \frac{\beta}{2}|\sigma|_{p}$$
subject to
$$F(u_g, \sigma, g) = 0.$$&lt;/p&gt;

&lt;p&gt;The Lagrangian multiplier method gives
$$\min_{\sigma, u_g, \lambda}L(\sigma, u_g, \lambda_g) =  J(\sigma, u_g) - \sum_g\lambda_g F_g$$&lt;/p&gt;

&lt;p&gt;The KKT conditon says
$$J_{\sigma} = \sum_g \lambda_g F_{g,\sigma},\quad J_{u_g} = \sum_{g}\lambda_g F_{g, u_g}$$&lt;/p&gt;

&lt;p&gt;To solve the problem, we use some penalty term in addition to get &lt;code&gt;AL&lt;/code&gt; as
$$L_{aug}(\sigma, u_g, \lambda_g, \mu) = J(\sigma, u_g) - \sum_{g}\lambda_g F_g + \frac{\mu}{2}|F(u_g,\sigma, g)|^2$$&lt;/p&gt;

&lt;p&gt;Then KKT condition implies at $k$-th iteration,&lt;br&gt;
$$J_{\sigma} -(\sum_g\lambda^k_g - \mu^k F)F_{g,\sigma} = 0 ,\quad J_{u_g} -(\sum_g\lambda^k_g - \mu^k F)F_{u_g,\sigma} = 0 $$&lt;/p&gt;

&lt;p&gt;which implies that near optimal, we have
$$\lambda_g^{\ast} = \lambda^{k}_g - \mu^k F_g.$$
giving equivalent Bregman iteration as
$$\lambda_g^{k+1} = \lambda^{k}_g - \mu^k F_g.$$&lt;/p&gt;

&lt;h4 id=&quot;toc_1&quot;&gt;Bregman is faster ?&lt;/h4&gt;

&lt;p&gt;No matter what method, one problem is in common, we need to solve sub-problem of finding $(\sigma^k, u_g^k)$ each time.&lt;/p&gt;

&lt;p&gt;In unconstrained case, $u_g$ must be solution of $F$. In Bregman, it is not. The problem is unconstrained, variables will increase to $O(n + N)$, where $n$ is unknowns of $\sigma$ and $N$ is unknowns of $u_g$.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s put everything into Bregman&amp;#39;s frame, set $u = (\sigma, u_g)$ and $J(u)$ defined in previous part is convex in $u$ and penalty term is
$$H(u) = \frac{\mu}{2}|F(u, g)|^2$$&lt;/p&gt;

&lt;p&gt;We are solving
$$\min_u {J(u) + H(u)}$$&lt;/p&gt;

&lt;p&gt;with Bregman distance $D^p_J(u, v) = J(u) - J(v) - \langle p , u-v\rangle$,the iteration is given by&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$u^{k+1} = \arg\min_{u} D_J^{p^k}(u , u^k) + H(u)$&lt;/li&gt;
&lt;li&gt;$p^{k+1} = p^k - \nabla H(u^{k+1})\in \partial J(u^{k+1})$&lt;/li&gt;
&lt;li&gt;update $\mu$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So what is the complexity in each iteration? In first update, we solve a minimization problem by BFGS/Newton. This problem is not easier than original problem at all. But adding $\mu$ makes the system less ill-conditioned, so finding a solution needs less iterations, while total iteration number might be large.&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/07/25/Bregman-as-default/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/07/25/Bregman-as-default/</guid>
      </item>
    
      <item>
        <title>Jenkins For Simulations</title>
        <description>&lt;p&gt;It is a note about how to make numerical simulations easier.&lt;/p&gt;

&lt;p&gt;I recently found that using some continuous integration pipeline for simulations is quite convenient. Softwares like Jenkins can organize the cluster through &lt;code&gt;ssh&lt;/code&gt; and try to balance all the loads.&lt;/p&gt;

&lt;p&gt;The setting up is quite easy, there are tutorials anywhere.&lt;/p&gt;

&lt;p&gt;For numerical simulations, all the different settings should be built in configuration file style or passed through stdin and taken as input instead built in binary executable. In this way, Jenkins run the simulations easily without manually setups.&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Jul 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/07/23/jenkins-for-simulation/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/07/23/jenkins-for-simulation/</guid>
      </item>
    
      <item>
        <title>Some Math Background in Biology</title>
        <description>&lt;p&gt;In terms of biology, here we only focus on molecule level. The molecules/proteins are clusters of atoms with bonds for interactions. The structure of protein could be quite different from each other and topologically complicated. Most structures are detected by X-ray, which is called X-ray crystallography.&lt;/p&gt;

&lt;p&gt;Crystals are periodical heterogeneous media under certain conditions. When X-ray passes through crystal, we will get diffraction pattern in 3D using various plane sensors for example. The light propagation is modeled as waves (geometric optics), and scattered by the crystal internal electrons.&lt;/p&gt;

&lt;p&gt;The patterns are simply Fourier transform of the density function, however, only magnitudes are captured. We will have to deal with a &lt;code&gt;phase recovery problem&lt;/code&gt;, this problem turns out to be an important and interesting topic in signal processing.&lt;/p&gt;

&lt;p&gt;When we are aware of the structure of such molecules, for many researchers, they are interested in the deformation of molecules/proteins, these motions of atoms are driven by some force field, computationally, it is called molecular dynamics (MD).&lt;/p&gt;

&lt;p&gt;This problem involved highly intensive numerical computing. There are several existing models. Solving Poisson Boltzmann equation is one of the tasks. To evaluate the potential on each atom requires solving the &lt;code&gt;PBE&lt;/code&gt;, which is non-linear in general, but there are linear approximation under certain condition. A popular method is to re-formulate as boundary integral equation and solve with fast solvers.&lt;/p&gt;

&lt;p&gt;The first kind integral equation is generally ill-conditioned, many researchers are shifting to second kind integral equation by differentiate the integral equation against target locations, however, this integral involves hyper-singular kernel and increase computational for each evaluation of integral.&lt;/p&gt;

&lt;p&gt;If we are only interested in solving the problem, there are some direct method for factorization of such dense matrix (HSS), which compresses the matrix with very few information, but without using expensive &lt;code&gt;svd&lt;/code&gt;.  While if we are interested in dynamics, then we have to take some extra cost in mind: for mesh dependent methods, re-meshing is necessary during each time step.&lt;/p&gt;

&lt;p&gt;For (weakly) singular integrals, when the mesh is changed, those diagonal values (pre-computed quadrature) will be re-computed again. Not mentioning the compression time.&lt;/p&gt;

&lt;p&gt;Hence, a more intriguing problem rises as:&lt;/p&gt;

&lt;p&gt;Operator $K$ is compressible with some algorithm (e.g. FMM or MG), if there is (global but possibly small) update to $K\to K&amp;#39;$, hopefully $K&amp;#39;$ has higher rank, but still compressible. (in terms of some other basis). Is there a fast algorithm for such update operation.&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/06/20/some-background-information/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/06/20/some-background-information/</guid>
      </item>
    
      <item>
        <title>Maxwell's Equations</title>
        <description>&lt;p&gt;This is a note for Maxwell&amp;#39;s equations (ME), ME is made up by 4 equations: Faraday&amp;#39;s law, Ampere&amp;#39;s law, Gauss&amp;#39; electric law, Gauss&amp;#39; magnetic law.&lt;/p&gt;

&lt;p&gt;[to be CONTINUED].&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Jun 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/06/04/maxwell-equation/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/06/04/maxwell-equation/</guid>
      </item>
    
      <item>
        <title>Quadrature Rule</title>
        <description>&lt;p&gt;For many PDE problems, they have some integral equation representation form too. I once mentioned whether it is good or not to put problem into integral form or not. For a lot of times, I think explicit form of something like Green&amp;#39;s function is necessary for such a solution
$$u = \int G(x, y)f(y)$$
over surface or volume.&lt;/p&gt;

&lt;p&gt;For smooth kernel function $G(x,y)$ in both variable, satisfying mild growth of derivatives magnitude. The local Taylor expansion could give sufficient high accuracy.&lt;/p&gt;

&lt;p&gt;For non-smooth kernel function, say piecewise defined function or globally $C^k(D)\times C^k(D)$, then it is not possible to approximate $G$ with high order polynomials. In the other word, the accuracy is limited to the discretization and its regularity.&lt;/p&gt;

&lt;p&gt;When $G(x,y) \in C^k(D)\times C^k(D)$, we can take derivatives upto $k$-th order for each of the variables, meaning that locally we have a reminder like $O(h^{k+1})$ near the irregular part.&lt;/p&gt;

&lt;p&gt;For instance, if our function $G(x, y) = |x-y|$ for volumetric integral, then $k=0$, and irregular part happens only at $x=y$, for uniform discretization of size $h$, the local error is at order $O(h)$. We can expect the integrated error to be $O(h^{d+1})$ provided the dimension is $d$. The convergence rate is expected to be $d+1$.&lt;/p&gt;

&lt;p&gt;For other cases such that singularity exists, e.g. $G(x,y) = |x-y|^{-1}$, special care has to be applied to the near interactions, which is far more tricky and expensive than irregular cases. Here the usual way is to divide the integral into several parts according to the &amp;quot;magnitude&amp;quot; of steepness. For those mild change parts, smoothness is preserved; for nearby parts, the change is faster, so divide into pieces to reduce the change rate; for the singular part, coordinate transform is applied.&lt;/p&gt;

&lt;p&gt;At the end of the day, it is all about tricks, there is no general rule for every kernel function, the precomputed quadrature has to be done for all geometries and all quadrature points. It should be also noticed that for those translation invariant kernel functions like $G(x,y) = G(|x-y|)$, the precomputation could be reused for similar (e.g. $G(\lambda t) = \lambda^{\beta} G(t)$) or the same geometries under rotations.&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/04/26/quadrature-rule/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/04/26/quadrature-rule/</guid>
      </item>
    
      <item>
        <title>PAT in 3D</title>
        <description>&lt;p&gt;It is quite challenging to numerically implement PAT in 3D. Especially for very high resolution imaging reconstruction.&lt;/p&gt;

&lt;p&gt;Even for solving the photon diffusion part is nontrivial on a grid of &lt;code&gt;512x512x512&lt;/code&gt;. To stably get recovery in diffusion coefficient, we need to solve the multi-source problem to at least third order, which means &lt;code&gt;H&lt;/code&gt; must be non-vanishing and accurate up to error of &lt;code&gt;h^3&lt;/code&gt;. If we are solving the equation with finite element forward solver, it is not cheap at all. The system will involve millions of dofs, only supercomputer or cluster could help.&lt;/p&gt;

&lt;p&gt;On our laptop, it is still unclear that whether a mesh larger than &lt;code&gt;64x64x64&lt;/code&gt; could work under second order FE smoothly. Down-sampling from the data, it is still reasonable to get some reconstruction up to some resolution.&lt;/p&gt;

&lt;p&gt;All we need is a fast forward solver, with parallelism, an option will be &lt;code&gt;PETSc&lt;/code&gt;. The detailed implementation could be any, &lt;code&gt;fenics&lt;/code&gt; is well-built for finite element solvers on top of &lt;code&gt;PETSc&lt;/code&gt;, could run in parallel over many cores.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;MATLAB&lt;/code&gt; will be a tough choice to handle too large system, it does not provide good support on MPI either. &lt;code&gt;Julia&lt;/code&gt; has some good libraries and graph tools, but not as many as &lt;code&gt;Python&lt;/code&gt; currently does.&lt;/p&gt;

&lt;p&gt;An interface prototype is needed then. [TO BE CONTINUED]&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Apr 2017 00:00:00 -0400</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/04/24/PAT-in-3D/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/04/24/PAT-in-3D/</guid>
      </item>
    
      <item>
        <title>Coding with Sparse Matrix</title>
        <description>&lt;p&gt;The difficulty in handling sparse matrix in coding is always underestimated. In &lt;code&gt;MATLAB&lt;/code&gt;, there is nothing extra to do with sparse matrix, because the software has taken care of it. For other languages like &lt;code&gt;C++&lt;/code&gt;, a fast and robust sparse matrix library is quite necessary for radiative transport equation, since each ray-tracing forms a extremely sparse matrix. And it will involve intense additions for sparse matrices.&lt;/p&gt;

&lt;p&gt;There are several popular libraries for sparse matrix operations. &lt;code&gt;csparse&lt;/code&gt; is one of it, using traditional &lt;code&gt;CSC&lt;/code&gt; or &lt;code&gt;CSR&lt;/code&gt; format to store the matrix.&lt;/p&gt;

&lt;p&gt;TO BE CONTINUED.&lt;/p&gt;
</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/01/03/coding-with-sparse-matrix/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/01/03/coding-with-sparse-matrix/</guid>
      </item>
    
      <item>
        <title>PML for 3D Wave Equation</title>
        <description>&lt;p&gt;In previous post, the wave equation in 2D, with PML for absorption boundary condition. The system is augmented to 4 unknown variables. And in 3D case, regardless of possible issues from PML, the system should involve more unknowns for additional dimension, however, it is actually more than that. It will bring an $s^{-1}$ term, representing an temporal integral.&lt;/p&gt;

&lt;p&gt;By Laplace transform (or Fourier transform) to frequency domain, the equation is simply as ($s$ is complex)&lt;/p&gt;

&lt;p&gt;$$s^2 u  = c^2 \Delta u$$&lt;/p&gt;

&lt;p&gt;By changing coordinate for each axis (e.g. $x, y, z$),
$$\tilde{x} = x + \displaystyle\int_0^x \sigma_x(\xi) d\xi$$&lt;/p&gt;

&lt;p&gt;we will arrive at a new system&lt;/p&gt;

&lt;p&gt;$$(s^2 + s(\sigma_x + \sigma_y + \sigma_z) + (\sigma_x\sigma_y + \sigma_y\sigma_z + \sigma_z \sigma_x) + \sigma_x\sigma_y\sigma_z s^{-1}) u = c^2 \Delta u + \nabla\cdot \Phi$$&lt;/p&gt;

&lt;p&gt;where $\Phi = (\phi^1, \phi^2, \phi^3)$ are auxiliary functions, inverting $s$ to $\partial_t$, we put the equation back to time domain,&lt;/p&gt;

&lt;p&gt;$$u_{tt} + p_1 u_t + p_2u + p_3 U = c^2 \Delta u + \nabla\cdot \Phi$$&lt;/p&gt;

&lt;p&gt;$$\Phi_t + \Sigma \Phi = c^2 (p_1 \mathbb{I} - \Sigma)\nabla u + \Gamma \nabla U$$&lt;/p&gt;

&lt;p&gt;$$U_t = u$$&lt;/p&gt;

&lt;p&gt;where $\Sigma = \mathrm{diag}(\sigma_x, \sigma_y, \sigma_z)$ and $\Gamma = \mathrm{diag}(\sigma_y\sigma_z, \sigma_z\sigma_x, \sigma_x\sigma_y)$. $p_1= \mathrm{Tr}(\Sigma)$, $p_2 = \mathrm{Tr}(\Gamma)$, $p_3 = \mathrm{det}(\Sigma)$.&lt;/p&gt;

&lt;p&gt;There are 6 unknowns to solve : $Y = (u, u_t, \phi^1, \phi^2, \phi^3, U)$. The system&amp;#39;s initial condition is from wave equation, additional variables are initialized as zero.&lt;/p&gt;

&lt;p&gt;$$Y_t = L Y$$&lt;/p&gt;

&lt;p&gt;can be solved with various numerical methods, $L$ is a second order operator in spatial variables, thus we can use FDM, FEM, pseudo-spectral.&lt;/p&gt;

&lt;p&gt;Pseudo-spectral is slower in complexity, but it will involve less points, since it is more accurate on the derivatives. The system needs evaluation on $U_x, U_y, U_z, u_x, u_y, u_z, c^2\Delta u, \nabla\cdot \Phi$ respectively, which requires &lt;code&gt;fft&lt;/code&gt; for 5 times on $u, U, \Phi$ and &lt;code&gt;ifft&lt;/code&gt; for 8 times, each &lt;code&gt;fft/ifft&lt;/code&gt; in theory needs $15Nlog_2N$ in 3D, thus total flops are $195 N\log_2 N$ flops, for a grid as large as 200x200x200, the total flops will be &lt;code&gt;7E10&lt;/code&gt; or so. On a single core machine at effective frequency 2.0GHz, the time will be &lt;code&gt;30s&lt;/code&gt; for one evaluation! For multi-core (say quad-core) platform, it will require (maybe) &lt;code&gt;10s&lt;/code&gt; for one run, 1000 time-steps with forward Euler will be about 3h, multi-step methods like RK2, RK3, RK4, the time will be much longer.&lt;/p&gt;

&lt;p&gt;If precision is not important, using &lt;code&gt;single&lt;/code&gt; precision will cut the timing in half.&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Dec 2016 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2016/12/23/PML-for-3D-wave-equation/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2016/12/23/PML-for-3D-wave-equation/</guid>
      </item>
    
  </channel>
</rss>