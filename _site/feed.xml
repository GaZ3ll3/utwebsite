<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Welcome to Yimin's Homepage</title>
    <description>Academic homepage of Yimin</description>
    <link>http://www.ma.utexas.edu/users/yzhong</link>
    <atom:link href="http://www.ma.utexas.edu/users/yzhong/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Bregman As Default</title>
        <description>&lt;p&gt;For most nonlinear inverse problem, direct solution is not accessible easily, thus many people chose an iterative optimization scheme.&lt;/p&gt;

&lt;p&gt;Suppose forward model $\mathcal{M}: (\sigma, u_g, g)\to m$ passes unknown variable coefficient $\sigma\in \mathcal{E}$, unknown solution $u_g$ and known input $g\in\mathcal{G}$ to some observable measurement $m$, where $u$ satisfies solution model constraint $F(u_g, \sigma, g) = 0$. we usually use minimization scheme:
$$\min_{\sigma} J(\sigma) = \sum_{g\in\mathcal{G}} |\mathcal{M}(\sigma, u_g, g) - m|^q + \frac{\beta}{2}|\sigma|_{p}$$&lt;/p&gt;

&lt;p&gt;values for $p,q$ are found everywhere, common choices for regularization is TV or $L_1, L_2$.&lt;/p&gt;

&lt;p&gt;When $q=2$, adjoint method is often used to reduce computational cost, ignoring $g$ for each input,&lt;/p&gt;

&lt;p&gt;$$\frac{\partial |\mathcal{M}(\sigma,u) - m|^2}{\partial \sigma} (\delta \sigma)=2( \frac{\partial \mathcal{M}}{\partial \sigma}(\delta \sigma))^t(\mathcal{M}(\sigma,u) - m)$$
and
$$\frac{\partial \mathcal{M}(\sigma, u)}{\partial \sigma}(\delta \sigma) = \mathcal{M}_{\sigma}\delta\sigma + \mathcal{M}_{u}\frac{\partial u}{\partial \sigma}(\delta \sigma)$$&lt;/p&gt;

&lt;p&gt;we need to resolve $\frac{\partial u}{\partial \sigma}$ which is reduced from solution model that
$$F_u \frac{\partial u}{\partial \sigma}(\delta \sigma) + F_{\sigma}(\delta\sigma) = 0$$&lt;/p&gt;

&lt;p&gt;so we have
$$\frac{\partial \mathcal{M}(\sigma, u)}{\partial \sigma}(\delta \sigma) = \mathcal{M}_{\sigma}\delta\sigma + \mathcal{M}_u F_u^{-1} F_{\sigma}(\delta\sigma)$$&lt;/p&gt;

&lt;p&gt;The $F^{-1}_u$ is some solution operator, we take adjoint, then we only need solve
$$(F_{\sigma}(\delta\sigma))^t F_u^{-t} \mathcal{M}_u^t (\mathcal{M}(\sigma, u) - m)$$&lt;/p&gt;

&lt;p&gt;Now, for each iteration, we need to evaluate operations, mostly matrix-vector multiplications.&lt;/p&gt;

&lt;p&gt;$$\mathcal{M}_u^t(y), F_u^{-t}(y), F_{\sigma}(\delta \sigma), F^{-1}(y)$$&lt;/p&gt;

&lt;p&gt;For linear operators, we have $F_u = F$, but $F_{\sigma}$ has to be calculated separately, for large system, $F^{-1}$ is nontrivial to update fast.&lt;/p&gt;

&lt;p&gt;This is the unconstrained version. This version of numerical implementation has to calculate exact solutions to $F$ multiple times $O(1)$.&lt;/p&gt;

&lt;p&gt;In order to quantify the cost, we consider following model of time cost.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$t_{mv}$ is sparse matrix-vector multiplication time.&lt;/li&gt;
&lt;li&gt;$t_{sv}$ is the inversion time.&lt;/li&gt;
&lt;li&gt;$t_{pre}$ is precomputing time for &lt;code&gt;mv&lt;/code&gt; or &lt;code&gt;sv&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;$t_{set}$ is total intrinsic setting time for all operators, which is minimal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Total time for each iteration will be: $t_{set} + t_{pre} + 2t_{sv} + (n+1)t_{mv}$. When the operators are sparse (local), then $t_{mv} = O(n)$, $t_{sv} = O(n)$, but mostly ill-conditioned, preconditioner is needed, total is $O(n^2)$. In this case, it is not optimal to use the low rank compression here due to high overhead in $t_{pre}$.&lt;/p&gt;

&lt;p&gt;If $F$ is integral equation, then system is dense. $F_u^{-1}$ or $F^{-1}$ is not cheap to calculate in practice, iterative method might be utilized for such problems, in such case, low rank method can kick in.&lt;/p&gt;

&lt;p&gt;Then we look at constrained version. Optimization problem is
$$\min_{\sigma, u_g} J(\sigma, u_g) = \sum_{g\in\mathcal{G}} |\mathcal{M}(\sigma, u_g, g) - m|^q + \frac{\beta}{2}|\sigma|_{p}$$
subject to
$$F(u_g, \sigma, g) = 0.$$&lt;/p&gt;

&lt;p&gt;The Lagrangian multiplier method gives
$$\min_{\sigma, u_g, \lambda}L(\sigma, u_g, \lambda_g) =  J(\sigma) - \sum_g\lambda_g F_g$$&lt;/p&gt;

&lt;p&gt;The KKT conditon says
$$J_{\sigma} = \sum_g \lambda_g F_{g,\sigma},\quad J_{u_g} = \sum_{g}\lambda_g F_{g, u_g}$$&lt;/p&gt;

&lt;p&gt;To solve the problem, we use some penalty term in addition to get &lt;code&gt;AL&lt;/code&gt; as
$$L_{al}(\sigma, u_g, \lambda_g, \mu) = J(\sigma) - \sum_{g}\lambda_g F_g + \frac{\mu}{2}|F(u_g,\sigma, g)|^2$$&lt;/p&gt;

&lt;p&gt;[TO BE CONTINUED]&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Jul 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/07/25/Bregman-as-default/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/07/25/Bregman-as-default/</guid>
      </item>
    
      <item>
        <title>Jenkins For Simulations</title>
        <description>&lt;p&gt;It is a note about how to make numerical simulations easier.&lt;/p&gt;

&lt;p&gt;I recently found that using some continuous integration pipeline for simulations is quite convenient. Softwares like Jenkins can organize the cluster through &lt;code&gt;ssh&lt;/code&gt; and try to balance all the loads.&lt;/p&gt;

&lt;p&gt;The setting up is quite easy, there are tutorials anywhere.&lt;/p&gt;

&lt;p&gt;For numerical simulations, all the different settings should be built in configuration file style or passed through stdin and taken as input instead built in binary executable. In this way, Jenkins run the simulations easily without manually setups.&lt;/p&gt;
</description>
        <pubDate>Sun, 23 Jul 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/07/23/jenkins-for-simulation/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/07/23/jenkins-for-simulation/</guid>
      </item>
    
      <item>
        <title>Some Math Background in Biology</title>
        <description>&lt;p&gt;In terms of biology, here we only focus on molecule level. The molecules/proteins are clusters of atoms with bonds for interactions. The structure of protein could be quite different from each other and topologically complicated. Most structures are detected by X-ray, which is called X-ray crystallography.&lt;/p&gt;

&lt;p&gt;Crystals are periodical heterogeneous media under certain conditions. When X-ray passes through crystal, we will get diffraction pattern in 3D using various plane sensors for example. The light propagation is modeled as waves (geometric optics), and scattered by the crystal internal electrons.&lt;/p&gt;

&lt;p&gt;The patterns are simply Fourier transform of the density function, however, only magnitudes are captured. We will have to deal with a &lt;code&gt;phase recovery problem&lt;/code&gt;, this problem turns out to be an important and interesting topic in signal processing.&lt;/p&gt;

&lt;p&gt;When we are aware of the structure of such molecules, for many researchers, they are interested in the deformation of molecules/proteins, these motions of atoms are driven by some force field, computationally, it is called molecular dynamics (MD).&lt;/p&gt;

&lt;p&gt;This problem involved highly intensive numerical computing. There are several existing models. Solving Poisson Boltzmann equation is one of the tasks. To evaluate the potential on each atom requires solving the &lt;code&gt;PBE&lt;/code&gt;, which is non-linear in general, but there are linear approximation under certain condition. A popular method is to re-formulate as boundary integral equation and solve with fast solvers.&lt;/p&gt;

&lt;p&gt;The first kind integral equation is generally ill-conditioned, many researchers are shifting to second kind integral equation by differentiate the integral equation against target locations, however, this integral involves hyper-singular kernel and increase computational for each evaluation of integral.&lt;/p&gt;

&lt;p&gt;If we are only interested in solving the problem, there are some direct method for factorization of such dense matrix (HSS), which compresses the matrix with very few information, but without using expensive &lt;code&gt;svd&lt;/code&gt;.  While if we are interested in dynamics, then we have to take some extra cost in mind: for mesh dependent methods, re-meshing is necessary during each time step.&lt;/p&gt;

&lt;p&gt;For (weakly) singular integrals, when the mesh is changed, those diagonal values (pre-computed quadrature) will be re-computed again. Not mentioning the compression time.&lt;/p&gt;

&lt;p&gt;Hence, a more intriguing problem rises as:&lt;/p&gt;

&lt;p&gt;Operator $K$ is compressible with some algorithm (e.g. FMM or MG), if there is (global but possibly small) update to $K\to K&amp;#39;$, hopefully $K&amp;#39;$ has higher rank, but still compressible. (in terms of some other basis). Is there a fast algorithm for such update operation.&lt;/p&gt;
</description>
        <pubDate>Tue, 20 Jun 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/06/20/some-background-information/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/06/20/some-background-information/</guid>
      </item>
    
      <item>
        <title>Maxwell's Equations</title>
        <description>&lt;p&gt;This is a note for Maxwell&amp;#39;s equations (ME), ME is made up by 4 equations: Faraday&amp;#39;s law, Ampere&amp;#39;s law, Gauss&amp;#39; electric law, Gauss&amp;#39; magnetic law.&lt;/p&gt;

&lt;p&gt;[to be CONTINUED].&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Jun 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/06/04/maxwell-equation/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/06/04/maxwell-equation/</guid>
      </item>
    
      <item>
        <title>Quadrature Rule</title>
        <description>&lt;p&gt;For many PDE problems, they have some integral equation representation form too. I once mentioned whether it is good or not to put problem into integral form or not. For a lot of times, I think explicit form of something like Green&amp;#39;s function is necessary for such a solution
$$u = \int G(x, y)f(y)$$
over surface or volume.&lt;/p&gt;

&lt;p&gt;For smooth kernel function $G(x,y)$ in both variable, satisfying mild growth of derivatives magnitude. The local Taylor expansion could give sufficient high accuracy.&lt;/p&gt;

&lt;p&gt;For non-smooth kernel function, say piecewise defined function or globally $C^k(D)\times C^k(D)$, then it is not possible to approximate $G$ with high order polynomials. In the other word, the accuracy is limited to the discretization and its regularity.&lt;/p&gt;

&lt;p&gt;When $G(x,y) \in C^k(D)\times C^k(D)$, we can take derivatives upto $k$-th order for each of the variables, meaning that locally we have a reminder like $O(h^{k+1})$ near the irregular part.&lt;/p&gt;

&lt;p&gt;For instance, if our function $G(x, y) = |x-y|$ for volumetric integral, then $k=0$, and irregular part happens only at $x=y$, for uniform discretization of size $h$, the local error is at order $O(h)$. We can expect the integrated error to be $O(h^{d+1})$ provided the dimension is $d$. The convergence rate is expected to be $d+1$.&lt;/p&gt;

&lt;p&gt;For other cases such that singularity exists, e.g. $G(x,y) = |x-y|^{-1}$, special care has to be applied to the near interactions, which is far more tricky and expensive than irregular cases. Here the usual way is to divide the integral into several parts according to the &amp;quot;magnitude&amp;quot; of steepness. For those mild change parts, smoothness is preserved; for nearby parts, the change is faster, so divide into pieces to reduce the change rate; for the singular part, coordinate transform is applied.&lt;/p&gt;

&lt;p&gt;At the end of the day, it is all about tricks, there is no general rule for every kernel function, the precomputed quadrature has to be done for all geometries and all quadrature points. It should be also noticed that for those translation invariant kernel functions like $G(x,y) = G(|x-y|)$, the precomputation could be reused for similar (e.g. $G(\lambda t) = \lambda^{\beta} G(t)$) or the same geometries under rotations.&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Apr 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/04/26/quadrature-rule/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/04/26/quadrature-rule/</guid>
      </item>
    
      <item>
        <title>PAT in 3D</title>
        <description>&lt;p&gt;It is quite challenging to numerically implement PAT in 3D. Especially for very high resolution imaging reconstruction.&lt;/p&gt;

&lt;p&gt;Even for solving the photon diffusion part is nontrivial on a grid of &lt;code&gt;512x512x512&lt;/code&gt;. To stably get recovery in diffusion coefficient, we need to solve the multi-source problem to at least third order, which means &lt;code&gt;H&lt;/code&gt; must be non-vanishing and accurate up to error of &lt;code&gt;h^3&lt;/code&gt;. If we are solving the equation with finite element forward solver, it is not cheap at all. The system will involve millions of dofs, only supercomputer or cluster could help.&lt;/p&gt;

&lt;p&gt;On our laptop, it is still unclear that whether a mesh larger than &lt;code&gt;64x64x64&lt;/code&gt; could work under second order FE smoothly. Down-sampling from the data, it is still reasonable to get some reconstruction up to some resolution.&lt;/p&gt;

&lt;p&gt;All we need is a fast forward solver, with parallelism, an option will be &lt;code&gt;PETSc&lt;/code&gt;. The detailed implementation could be any, &lt;code&gt;fenics&lt;/code&gt; is well-built for finite element solvers on top of &lt;code&gt;PETSc&lt;/code&gt;, could run in parallel over many cores.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;MATLAB&lt;/code&gt; will be a tough choice to handle too large system, it does not provide good support on MPI either. &lt;code&gt;Julia&lt;/code&gt; has some good libraries and graph tools, but not as many as &lt;code&gt;Python&lt;/code&gt; currently does.&lt;/p&gt;

&lt;p&gt;An interface prototype is needed then. [TO BE CONTINUED]&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Apr 2017 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/04/24/PAT-in-3D/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/04/24/PAT-in-3D/</guid>
      </item>
    
      <item>
        <title>Coding with Sparse Matrix</title>
        <description>&lt;p&gt;The difficulty in handling sparse matrix in coding is always underestimated. In &lt;code&gt;MATLAB&lt;/code&gt;, there is nothing extra to do with sparse matrix, because the software has taken care of it. For other languages like &lt;code&gt;C++&lt;/code&gt;, a fast and robust sparse matrix library is quite necessary for radiative transport equation, since each ray-tracing forms a extremely sparse matrix. And it will involve intense additions for sparse matrices.&lt;/p&gt;

&lt;p&gt;There are several popular libraries for sparse matrix operations. &lt;code&gt;csparse&lt;/code&gt; is one of it, using traditional &lt;code&gt;CSC&lt;/code&gt; or &lt;code&gt;CSR&lt;/code&gt; format to store the matrix.&lt;/p&gt;

&lt;p&gt;TO BE CONTINUED.&lt;/p&gt;
</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 -0600</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2017/01/03/coding-with-sparse-matrix/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2017/01/03/coding-with-sparse-matrix/</guid>
      </item>
    
      <item>
        <title>PML for 3D Wave Equation</title>
        <description>&lt;p&gt;In previous post, the wave equation in 2D, with PML for absorption boundary condition. The system is augmented to 4 unknown variables. And in 3D case, regardless of possible issues from PML, the system should involve more unknowns for additional dimension, however, it is actually more than that. It will bring an $s^{-1}$ term, representing an temporal integral.&lt;/p&gt;

&lt;p&gt;By Laplace transform (or Fourier transform) to frequency domain, the equation is simply as ($s$ is complex)&lt;/p&gt;

&lt;p&gt;$$s^2 u  = c^2 \Delta u$$&lt;/p&gt;

&lt;p&gt;By changing coordinate for each axis (e.g. $x, y, z$),
$$\tilde{x} = x + \displaystyle\int_0^x \sigma_x(\xi) d\xi$$&lt;/p&gt;

&lt;p&gt;we will arrive at a new system&lt;/p&gt;

&lt;p&gt;$$(s^2 + s(\sigma_x + \sigma_y + \sigma_z) + (\sigma_x\sigma_y + \sigma_y\sigma_z + \sigma_z \sigma_x) + \sigma_x\sigma_y\sigma_z s^{-1}) u = c^2 \Delta u + \nabla\cdot \Phi$$&lt;/p&gt;

&lt;p&gt;where $\Phi = (\phi^1, \phi^2, \phi^3)$ are auxiliary functions, inverting $s$ to $\partial_t$, we put the equation back to time domain,&lt;/p&gt;

&lt;p&gt;$$u_{tt} + p_1 u_t + p_2u + p_3 U = c^2 \Delta u + \nabla\cdot \Phi$$&lt;/p&gt;

&lt;p&gt;$$\Phi_t + \Sigma \Phi = c^2 (p_1 \mathbb{I} - \Sigma)\nabla u + \Gamma \nabla U$$&lt;/p&gt;

&lt;p&gt;$$U_t = u$$&lt;/p&gt;

&lt;p&gt;where $\Sigma = \mathrm{diag}(\sigma_x, \sigma_y, \sigma_z)$ and $\Gamma = \mathrm{diag}(\sigma_y\sigma_z, \sigma_z\sigma_x, \sigma_x\sigma_y)$. $p_1= \mathrm{Tr}(\Sigma)$, $p_2 = \mathrm{Tr}(\Gamma)$, $p_3 = \mathrm{det}(\Sigma)$.&lt;/p&gt;

&lt;p&gt;There are 6 unknowns to solve : $Y = (u, u_t, \phi^1, \phi^2, \phi^3, U)$. The system&amp;#39;s initial condition is from wave equation, additional variables are initialized as zero.&lt;/p&gt;

&lt;p&gt;$$Y_t = L Y$$&lt;/p&gt;

&lt;p&gt;can be solved with various numerical methods, $L$ is a second order operator in spatial variables, thus we can use FDM, FEM, pseudo-spectral.&lt;/p&gt;

&lt;p&gt;Pseudo-spectral is slower in complexity, but it will involve less points, since it is more accurate on the derivatives. The system needs evaluation on $U_x, U_y, U_z, u_x, u_y, u_z, c^2\Delta u, \nabla\cdot \Phi$ respectively, which requires &lt;code&gt;fft&lt;/code&gt; for 5 times on $u, U, \Phi$ and &lt;code&gt;ifft&lt;/code&gt; for 8 times, each &lt;code&gt;fft/ifft&lt;/code&gt; in theory needs $15Nlog_2N$ in 3D, thus total flops are $195 N\log_2 N$ flops, for a grid as large as 200x200x200, the total flops will be &lt;code&gt;7E10&lt;/code&gt; or so. On a single core machine at effective frequency 2.0GHz, the time will be &lt;code&gt;30s&lt;/code&gt; for one evaluation! For multi-core (say quad-core) platform, it will require (maybe) &lt;code&gt;10s&lt;/code&gt; for one run, 1000 time-steps with forward Euler will be about 3h, multi-step methods like RK2, RK3, RK4, the time will be much longer.&lt;/p&gt;

&lt;p&gt;If precision is not important, using &lt;code&gt;single&lt;/code&gt; precision will cut the timing in half.&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Dec 2016 00:00:00 -0600</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2016/12/23/PML-for-3D-wave-equation/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2016/12/23/PML-for-3D-wave-equation/</guid>
      </item>
    
      <item>
        <title>Simplified Spherical Harmonics</title>
        <description>&lt;p&gt;Simplified spherical harmonics has been proposed decades ago, and it provides an approximation to ERT, by generalizing differential operator $dx$ to $\nabla$. The solution can be obtained by solving a diffusion equation system, instead of solving just one diffusion equation by assuming diffusive regime, $SP_n$ can make corrections to diffusion system by capturing more details/modes.&lt;/p&gt;

&lt;p&gt;One thing to notice is, $SP_n$ is still an approximation even $n\to \infty$, since it is not solving the actual equation.  Usually, the scattering behavior is described with phase function, and a common choice is Henyey-Greenstein phase function.&lt;/p&gt;

&lt;p&gt;$$p(\mu, g) = \frac{1 - g^2}{4\pi (1 + g^2 - 2g\mu)^{3/2}}$$&lt;/p&gt;

&lt;p&gt;where $\mu = \Omega&amp;#39;\cdot \Omega$ as the scattering angle. And&lt;/p&gt;

&lt;p&gt;$$p(\mu, g) = \sum_{n=0}^{\infty} \frac{2n+1}{4\pi} g^n P_n(\mu)$$&lt;/p&gt;

&lt;p&gt;One interesting thing is to compare the idea behind integral-based method with reduced phase, and simplified spherical harmonics. If we follow the idea of $SP_n$, consider the 1D model and generalize to finite domain, then it is straightforward, because we just take azimuth angle and ignore the polar angle, completely achieves the same complexity, but model is slightly changed.&lt;/p&gt;

&lt;p&gt;Under forward-peaking regime with large scattering, this is  reasonable though.&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Nov 2016 00:00:00 -0600</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2016/11/10/simplified-spherical-harmonics/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2016/11/10/simplified-spherical-harmonics/</guid>
      </item>
    
      <item>
        <title>Time domain RTE</title>
        <description>&lt;p&gt;The time dependent radiative transport equation is stated as
$$
\begin{equation}
\begin{aligned}
&amp;amp;\frac{1}{c}\frac{\partial u}{\partial t} + \hat{s}\cdot \nabla u = \sigma(B(\nu, T) - u)\\
&amp;amp;C_{\nu}\frac{\partial T}{\partial t} = \int_{S^{d-1}}\int_0^{\infty} \sigma(u - B(\nu, T)) d\nu d\hat{s}
\end{aligned}
\end{equation}
$$
which describes the interaction of material and radiation. where $u(x, t, \hat{s},\nu)$ is radiation intensity, $T = T(x,t)$ is material temperature. $\sigma = \sigma(x, \nu, T)$ is opacity thickness, $C_{\nu}$ is heat capacity.
$$B(\nu, T) = \frac{2h}{c^3}\frac{\nu^3}{\exp(h\nu/k T) - 1}$$&lt;/p&gt;

&lt;p&gt;by using approximation, the equation can be rewritten as an easier one,
$$
\begin{equation}
\begin{aligned}
&amp;amp;\frac{1}{c}\frac{\partial u}{\partial t} + \hat{s}\cdot \nabla u + \sigma u = \frac{\sigma b}{|S^{d-1}|} acT^4\\
&amp;amp;C_{\nu}\frac{\partial T}{\partial t} = \int_{S^{d-1}}\int_0^{\infty} \sigma d\nu \int_{S^{d-1}} u \hat{s} - \sigma_p acT^4
\end{aligned}
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;where $b$ is average of $B$, $\sigma_p$ is average of $\sigma b$.&lt;/p&gt;

&lt;p&gt;There is a way to decouple the equations, by taking $t$ as another spatial variable. $z = (x, t)$, we have
$$
\tilde{s}\cdot \nabla_z u + \sigma(z, \nu) u = H(z, \nu)
$$&lt;/p&gt;

&lt;p&gt;which is quite easy to come up with a solution for $\phi = \int_{S^{d-1}} u$. And insert into temperature equation.&lt;/p&gt;

&lt;p&gt;$$C_{\nu}\frac{\partial T}{\partial t} = \int_0^{\infty}\sigma d\nu \int_{S^{d-1}}\int_0^{\tau^{-}(z,\tilde{s})} \exp(-\int_0^p\sigma(z-\mu\tilde{s})d\mu) H(z - p\tilde{s}) dp d\tilde{s} - \sigma_p ac T^4$$&lt;/p&gt;

&lt;p&gt;And solving this can apply some fast algorithm like &lt;code&gt;treecode&lt;/code&gt; or &lt;code&gt;FMM&lt;/code&gt; as we did before, forward Euler scheme will generate $O(\Delta x)$ error.&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Sep 2016 00:00:00 -0500</pubDate>
        <link>http://www.ma.utexas.edu/users/yzhong/2016/09/19/time-domain-RTE/</link>
        <guid isPermaLink="true">http://www.ma.utexas.edu/users/yzhong/2016/09/19/time-domain-RTE/</guid>
      </item>
    
  </channel>
</rss>